{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsLoUIOmEiF6"
   },
   "source": [
    "# 1. Setup and Dependencies\n",
    "\n",
    "Before we begin, we install the `datasets` library from Hugging Face to easily access the required dataset. We also import **PyTorch**, which we will be using as our deep learning framework.\n",
    "\n",
    "**Key Imports:**\n",
    "* `torch.nn`: Contains the building blocks for neural networks (LSTM layers, Linear layers, etc.).\n",
    "* `datasets`: Used to download the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD6va51KC67T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2KE5iaoE9B_"
   },
   "source": [
    "# 2. Data Loading & Preprocessing\n",
    "\n",
    "Since we are building a Character-Level LSTM, our \"vocabulary\" consists of individual characters (letters, punctuation, newlines) rather than full words.\n",
    "\n",
    "**What this cell does:**\n",
    "1.  **Loads WikiText-2:** This is the dataset by Salesforce we have chosen for our model, which is often used for language modeling benchmarks.\n",
    "2.  **Builds Vocabulary:** Scans the text to find all unique characters (approx. 200-300 unique chars).\n",
    "3.  **Numerical Encoding:** Creates a mapping (`char_to_idx`) to convert characters into integers, which is the format the neural network requires.\n",
    "\n",
    "### Understanding with an Example\n",
    "\n",
    "To visualize exactly what the code above does, let's pretend our entire `text` variable is just the word **\"mississippi\"**.\n",
    "\n",
    "#### 1. Extract & Sort Unique Characters\n",
    "The code `sorted(list(set(text)))` finds the unique letters and sorts them alphabetically.\n",
    "\n",
    "* **Raw Text:** `\"mississippi\"`\n",
    "* **Unique Set:** `{'m', 'i', 's', 'p'}`\n",
    "* **Sorted List (`chars`):** `['i', 'm', 'p', 's']`\n",
    "\n",
    "#### 2. Create Mappings (`char_to_idx`)\n",
    "We assign a unique integer to each character based on its sorted position.\n",
    "\n",
    "| Index | Character |\n",
    "| :--- | :--- |\n",
    "| 0 | **i** |\n",
    "| 1 | **m** |\n",
    "| 2 | **p** |\n",
    "| 3 | **s** |\n",
    "\n",
    "#### 3. Encoding the Text\n",
    "The function `encode_text` replaces every letter in the original string with its corresponding integer.\n",
    "\n",
    "* **Original:** `m` `i` `s` `s` `i` `s` `s` `i` `p` `p` `i`\n",
    "* **Encoded:** `1` `0` `3` `3` `0` `3` `3` `0` `2` `2` `0`\n",
    "\n",
    "**Result:** The model receives the tensor `[1, 0, 3, 3, 0, 3, 3, 0, 2, 2, 0]` instead of the raw text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9z-FCA8KUsF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPYe5TIUDK-t"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# Extract and join the train and validation splits into a single\n",
    "# long string each separated by newline characters '\\n'\n",
    "text = \"\\n\".join(dataset['train']['text'])\n",
    "val_text = \"\\n\".join(dataset['validation']['text'])\n",
    "\n",
    "# Create Vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "chars.append('<UNK>')\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Total characters in training set: {len(text)}\")\n",
    "print(f\"Unique characters (Vocabulary Size): {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "\n",
    "# Function to encode text to integers\n",
    "def encode_text(txt):\n",
    "    unk_idx = char_to_idx['<UNK>']\n",
    "    return torch.tensor([char_to_idx.get(c, unk_idx) for c in txt], dtype=torch.long)\n",
    "\n",
    "# Encode datasets\n",
    "train_data = encode_text(text)\n",
    "val_data = encode_text(val_text)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYSlDn0bLAQl"
   },
   "source": [
    "---\n",
    "# 3. **Your Task**: Define the LSTM Model\n",
    "\n",
    "In this step, you will define the architecture of the Recurrent Neural Network. We will use an **LSTM (Long Short-Term Memory)** network, since it is excellent at learning sequences.\n",
    "\n",
    "Your goal is to complete the `CharLSTM` class below. The model consists of three main components that transform the data step-by-step.\n",
    "\n",
    "#### 1. The Components (`__init__`)\n",
    "You need to define three layers:\n",
    "* **Embedding Layer:** This converts our simple integer inputs (like `1` for 'm') into dense, meaningful vectors. It turns a single number into a list of floating-point numbers that represent that character's \"meaning.\"\n",
    "* **LSTM Layer:** The core engine. It takes the embeddings and processes them sequentially, updating its internal \"memory\" (hidden state) at every step.\n",
    "* **Fully Connected (Linear) Layer:** This takes the output of the LSTM and maps it back to the size of our vocabulary. It produces a score for every possible next character, telling us which character is most likely to come next.\n",
    "\n",
    "#### 2. The Forward Pass (`forward`)\n",
    "This function defines how data flows through the network:\n",
    "1.  **Input:** Takes a batch of character indices (e.g., `[1, 0, 3...]` for \"miss...\").\n",
    "2.  **Embed:** Pass indices through the Embedding layer.\n",
    "3.  **Process:** Pass the embeddings into the LSTM layer along with the previous `hidden` state.\n",
    "4.  **Reshape:** Flatten the output so the Linear layer can process it (stacking the batch and sequence dimensions).\n",
    "5.  **Output:** Pass the flattened data through the Fully Connected layer to get prediction scores (logits).\n",
    "\n",
    "#### 3. Hidden State Initialization (`init_hidden`)\n",
    "LSTMs need a \"starting state\" (short-term and long-term memory) before they see any data. This function should create tensors of zeros for both the hidden state and cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNTA655kDMDl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        h0 = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        c0 = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ur9z47Mhrx"
   },
   "source": [
    "# 4. Helper Functions: Batching & Generation\n",
    "\n",
    "We need two utility functions to make training and testing easier:\n",
    "\n",
    "1.  **`get_batch`**: Instead of training on the whole text at once (which is too large), we grab random chunks (sequences).\n",
    "    * **Input (`x`):** A sequence of characters (e.g., \"Hell\").\n",
    "    * **Target (`y`):** The same sequence shifted by one (e.g., \"ello\"). This teaches the model to predict the next character.\n",
    "\n",
    "2.  **`generate_text`**: This function allows us to \"talk\" to the model.\n",
    "    * It feeds a starting string (prompt) into the model.\n",
    "    * It takes the predicted character, feeds it back in as the next input, and repeats the process.\n",
    "    * **Temperature Sampling:** It uses a \"temperature\" parameter to control randomness (explained in step 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-BYAWKODPOa"
   },
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, batch_size):\n",
    "\n",
    "    # Random starting points\n",
    "    ix = torch.randint(0, len(data) - seq_len - 1, (batch_size,))\n",
    "\n",
    "    # Stack inputs (x) and targets (y)\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix]) # Target is x shifted by 1\n",
    "\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def generate_text(model, start_str=\"The\", length=200, temperature=1.0):\n",
    "    model.eval() # Set to evaluation mode\n",
    "    input_idxs = encode_text(start_str).unsqueeze(0).to(device) # (1, seq_len)\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    generated_text = start_str\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Warm up hidden state with the start string\n",
    "        for i in range(len(start_str) - 1):\n",
    "            _, hidden = model(input_idxs[:, i:i+1], hidden)\n",
    "\n",
    "        # Generate new characters\n",
    "        last_char_idx = input_idxs[:, -1:]\n",
    "        for _ in range(length):\n",
    "            logits, hidden = model(last_char_idx, hidden)\n",
    "\n",
    "            # Apply temperature\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            last_char_idx = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Append to result\n",
    "            generated_char = idx_to_char[last_char_idx.item()]\n",
    "            generated_text += generated_char\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMG08qh6M4Bp"
   },
   "source": [
    "# 5. Training the Model\n",
    "\n",
    "This is the main loop where the model learns. We use the **Adam optimizer** and **CrossEntropyLoss**.\n",
    "\n",
    "**Key Concepts in this Loop:**\n",
    "* **Hidden State Initialization:** At the start of each random batch, we initialize the hidden state to zeros.\n",
    "* **Gradient Clipping (`clip_grad_norm_`):** RNNs and LSTMs can suffer from \"exploding gradients\" (where error signals become too large). We clip them to a maximum value of 5 to keep training stable.\n",
    "* **Validation:** Every 200 steps, we pause training to check the loss on unseen data (validation set) and generate a sample string. This helps us see if the model is actually learning English or just memorizing noise.\n",
    "---\n",
    "# **Your Task**: Hyperparameter Tuning\n",
    "Modify the parameters below to maximize performance.\n",
    "\n",
    "Experiment with different values to minimize validation loss by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpLPzbz8DRM5"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "\n",
    "# Define the hyperparameters\n",
    "vocab_size = len(chars)\n",
    "embed_size =\n",
    "hidden_size =\n",
    "n_layers =\n",
    "seq_len =\n",
    "batch_size =\n",
    "lr =\n",
    "epochs =\n",
    "print_every =\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# Initialize Model\n",
    "model = CharLSTM(vocab_size, embed_size, hidden_size, n_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tracking metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "for step in range(1, epochs + 1):\n",
    "    # Get batch\n",
    "    inputs, targets = get_batch(train_data, seq_len, batch_size)\n",
    "\n",
    "    # Init hidden state\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, hidden = model(inputs, hidden)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(output, targets.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients (common in RNNs)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    if step % print_every == 0:\n",
    "        # Validation Check\n",
    "        val_inputs, val_targets = get_batch(val_data, seq_len, batch_size)\n",
    "        val_hidden = model.init_hidden(batch_size)\n",
    "        val_output, _ = model(val_inputs, val_hidden)\n",
    "        val_loss = criterion(val_output, val_targets.view(-1))\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Step {step}/{epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "        print(f\"Sample Gen: {generate_text(model, start_str='The ', length=50, temperature=0.8)}\")\n",
    "        print(\"-\" * 50)\n",
    "        model.train() # Reset to train mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAf2YtCZjuRO"
   },
   "source": [
    "# 6. Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqk4rpY5Dpmf"
   },
   "outputs": [],
   "source": [
    "# Plotting Loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "# Scale validation x-axis to match training steps\n",
    "val_steps = [i * print_every for i in range(len(val_losses))]\n",
    "plt.plot(val_steps, val_losses, label='Validation Loss', linewidth=3)\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szxlWYwewlsf"
   },
   "source": [
    "# 7. Temperature Sampling & Probing Experiment\n",
    "\n",
    "Now comes the fun part! We will use our trained LSTM to generate completely new text character-by-character.\n",
    "\n",
    "### Understanding \"Temperature\"\n",
    "When the model predicts the next character, it assigns a probability to every letter in the alphabet. We don't always want to pick the \"most likely\" letter (that would be boring and repetitive). Instead, we **sample** from the list of possibilities.\n",
    "\n",
    "We use a hyperparameter called **Temperature (`T`)** to control how \"risky\" the model is allowed to be:\n",
    "\n",
    "* **Low Temperature ($T < 0.5$):** The model plays it safe. It makes the high probabilities even higher.\n",
    "    * *Result:* The text is grammatically correct but very conservative and repetitive.\n",
    "* **High Temperature ($T \\geq 1.0$):** The model flattens the probabilities, giving \"rare\" characters a fighting chance.\n",
    "    * *Result:* The text is more creative and diverse, but might contain spelling errors or nonsense words.\n",
    "\n",
    "Observation: The \"Unclosed Bracket\" Problem\n",
    "In the code below, when we set the seed with an open parenthesis, for example, `(The `. Watch closely to see if the model successfully closes the parenthesis `)` later in the paragraph.\n",
    "\n",
    "Does it successfully close the bracket, or does it fail to? Why do you think this happens?\n",
    "\n",
    "---\n",
    "# **Your Task**\n",
    "1.  Run the cell to see how the model behaves at different temperatures.\n",
    "2.  Experiment: Change the `seed_text` variable to something else (e.g., your name, \"The meaning of life is\", etc.).\n",
    "3.  Compare the output of Temperature 0.2 vs. 1.0. Which one feels more \"human\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UchdZGUdOhBZ"
   },
   "outputs": [],
   "source": [
    "def predict(model, char, h=None, top_k=None, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Given a character, predict the next character.\n",
    "    Returns the predicted character and the hidden state.\n",
    "    \"\"\"\n",
    "    # Prepare input\n",
    "    x = np.array([[char_to_idx[char]]])\n",
    "    x = torch.from_numpy(x)\n",
    "\n",
    "    # Move to device\n",
    "    # Check if the model's parameters are on GPU to determine device\n",
    "    device = next(model.parameters()).device\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # Get output from model\n",
    "    model.eval()\n",
    "    out, h = model(x, h)\n",
    "\n",
    "    # Apply temperature scaling to logits\n",
    "    # out shape is (batch, seq, vocab), we want the last step: out[0, -1, :]\n",
    "    out = out[-1]\n",
    "    prob = F.softmax(out / temperature, dim=0).data\n",
    "\n",
    "    # Sample from the distribution\n",
    "    # This introduces the \"randomness\" based on the probabilities\n",
    "    char_ind = torch.multinomial(prob, 1).item()\n",
    "\n",
    "    return idx_to_char[char_ind], h\n",
    "\n",
    "def sample(net, size, prime='The', temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text.\n",
    "    \"\"\"\n",
    "    device = next(net.parameters()).device\n",
    "    net.to(device)\n",
    "    net.eval() # Evaluation mode\n",
    "\n",
    "    # First, run the prime characters through to build up the hidden state\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1) # Batch size 1\n",
    "\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, temperature=temperature)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and generate new ones\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, temperature=temperature)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "# --- PROBING EXPERIMENT ---\n",
    "\n",
    "seed_text = \"(The \"  # Change this to whatever you want the model to start with\n",
    "gen_length = 400    # How many characters to generate\n",
    "\n",
    "print(f\"--- PROBING MODEL BEHAVIOR (Seed: '{seed_text}') ---\\n\")\n",
    "\n",
    "# 1. Low Temperature\n",
    "print(\"Temperature 0.2 (Safe/Repetitive):\")\n",
    "print(sample(model, gen_length, prime=seed_text, temperature=0.2))\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 2. Medium Temperature\n",
    "print(\"Temperature 0.5 (Balanced):\")\n",
    "print(sample(model, gen_length, prime=seed_text, temperature=0.5))\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# 3. High Temperature\n",
    "print(\"Temperature 1.0 (Creative/Risky):\")\n",
    "print(sample(model, gen_length, prime=seed_text, temperature=1.0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
